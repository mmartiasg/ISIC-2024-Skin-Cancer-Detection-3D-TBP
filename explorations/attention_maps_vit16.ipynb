{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T20:04:20.776503Z",
     "start_time": "2024-08-22T20:04:18.911067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "from prototypes.deeplearning.models import VitPrototype1Dropout\n",
    "from prototypes.utility.data import DataLoader\n",
    "from prototypes.utility.data import ProjectConfiguration"
   ],
   "id": "9b767302e0fc4304",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T20:04:21.057852Z",
     "start_time": "2024-08-22T20:04:21.055171Z"
    }
   },
   "cell_type": "code",
   "source": "config = ProjectConfiguration(\"../config.json\")",
   "id": "dd9077b369591e0",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T20:04:31.749703Z",
     "start_time": "2024-08-22T20:04:21.799105Z"
    }
   },
   "cell_type": "code",
   "source": "data_loader = DataLoader(data_path=config.get_value(\"TRAIN_IMAGES_PATH\"), metadata_path=config.get_value(\"TRAIN_METADATA\"))",
   "id": "190a66fe0352ac25",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T20:04:31.952873Z",
     "start_time": "2024-08-22T20:04:31.750780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cancer_images = data_loader.get_data(target=1, n_sample=300, width=128, height=128)\n",
    "non_cancer_images = data_loader.get_data(target=0, n_sample=300, width=128, height=128)"
   ],
   "id": "49c3b196f4713076",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T20:04:32.675055Z",
     "start_time": "2024-08-22T20:04:31.953777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "with torch.no_grad():\n",
    "    # inputs = torchvision.models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1.transforms()(Image.fromarray(cancer_images[3]))\n",
    "    # outputs = vit_model(inputs.unsqueeze(0))\n",
    "    inputs = torch.tensor(np.array([torchvision.models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1.transforms()(Image.fromarray(image)) for image in cancer_images]))\n",
    "    # outputs = model(torch.tensor(inputs))"
   ],
   "id": "ed60935f835a8a9",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T20:44:29.512746Z",
     "start_time": "2024-08-22T20:44:28.361046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# give image to the model and collect the attention maps\n",
    "model = VitPrototype1Dropout(n_classes=1)\n",
    "model.load_state_dict(torch.load(\"../checkpoint_resnet50_mix_up/0.1.1_vit16Dropout_best.pt\", weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "# Register hooks to capture the attention weights\n",
    "attention_maps = []\n",
    "def get_attention_weights(module, input, output):\n",
    "    # Store the attention weights\n",
    "    attention_maps.append(output[0])\n",
    "    # attention_maps.append(module.self_attention.attn_output_weights.deatch())\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torchvision.models.vision_transformer.EncoderBlock):\n",
    "        print(name)\n",
    "        module.self_attention.register_forward_hook(get_attention_weights)\n",
    "\n",
    "# Pass the image through the model\n",
    "with torch.no_grad():\n",
    "    output = model(inputs[0].unsqueeze(0))\n",
    "    # Convert attention maps to numpy arrays\n",
    "    attention_maps = [att.cpu().numpy() for att in attention_maps]"
   ],
   "id": "c3df75dfa2e87fd6",
   "execution_count": 41,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T20:45:55.402100Z",
     "start_time": "2024-08-22T20:45:54.490338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = VitPrototype1Dropout(n_classes=1)\n",
    "model.load_state_dict(torch.load(\"../checkpoint_resnet50_mix_up/0.1.1_vit16Dropout_best.pt\", weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "def get_attention_weights(module, input, output):\n",
    "    # Store the attention weights\n",
    "    attention_maps.append(output)\n",
    "\n",
    "handle = model.model.encoder.layers.encoder_layer_11.ln_1.register_forward_hook(get_attention_weights)\n",
    "model.eval()"
   ],
   "id": "bbcadc3934b351ca",
   "execution_count": 46,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T20:45:58.687813Z",
     "start_time": "2024-08-22T20:45:58.497515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "\n",
    "# Pass the image through the model\n",
    "with torch.no_grad():\n",
    "    # We get the output\n",
    "    output = model(inputs[0].unsqueeze(0))\n",
    "    # print(output.shape)\n",
    "\n",
    "    # So here we get the weights and biases for the quer, key, and value\n",
    "    qkv_w = model.model.encoder.layers.encoder_layer_11.self_attention.in_proj_weight\n",
    "    qkv_b = model.model.encoder.layers.encoder_layer_11.self_attention.in_proj_bias\n",
    "\n",
    "    print(f\"The shape of qkv weight matrix before reshaping is {qkv_w.shape}\\n\")\n",
    "    print(f\"The shape of qkv bias matrix before reshaping is {qkv_b.shape}\\n\")\n",
    "    # print(qkv_w.shape)\n",
    "    \"\"\"we have shape of (2304 * 768), we need to understand what is the meaning of the dimensions we have?\n",
    "    first of all, the 768 represnets the D-hidden dimension through the encoder of the vision transformer which is fiexd across all of the encoder network.\n",
    "    2304 is a little bit tricky and you need to check the original paper to understand why the shape looks like that.\n",
    "\n",
    "    We have 3 components (query, keys, and values) for each head, and at the encoder (Architecture dependent) we have 12 heads, then we explore this as first divide 2304 by 12 to get dimensions for each head = 2304/12 = 192, here remember that we have 3 matrices stacked so 192/3 = 64, \n",
    "    which is the dimension of the head mentioned in the paper as D_{h} = D/k, and K is the number of heads which is 12 for the vit_b_16()\"\"\"\n",
    "\n",
    "    #shape here is (matrices, d_head *k, d_hidden)\n",
    "    qkv_w = qkv_w.reshape(3, -1, 768)\n",
    "    qkv_b = qkv_b.reshape(12, -1, 64)\n",
    "\n",
    "    print(f\"The shape of qkv weight matrix after reshaping is {qkv_w.shape}\\n\")\n",
    "    print(f\"The shape of qkv bias matrix after reshaping is {qkv_b.shape}\\n\")\n",
    "\n",
    "    \"Here we get the weights and biases for each component for all of the heads\"\n",
    "    \n",
    "    #shape here for each weight component is (d_head *k, d_hidden)\n",
    "    q_w_12_heads = qkv_w[0,:,:]\n",
    "    k_w_12_heads = qkv_w[1,:,:]\n",
    "    v_w_12_heads = qkv_w[2,:,:]\n",
    "\n",
    "    \n",
    "\n",
    "    q_b_12_heads = qkv_b[:,0,:]\n",
    "    k_b_12_heads = qkv_b[:,1,:]\n",
    "    v_b_12_heads = qkv_b[:,2,:]\n",
    "\n",
    "\n",
    "    print(f\"The shape of query weight matrix before reshaping is {q_w_12_heads.shape}, key weight is {k_w_12_heads.shape}, and values weight is {v_w_12_heads.shape}\\n\")\n",
    "    print(f\"The shape of query bias matrix before reshaping is {q_b_12_heads.shape}, key bias is {k_b_12_heads.shape}, and values bias is {v_b_12_heads.shape}\\n\")\n",
    "\n",
    "    # Shape here is (no.head, d_head, d_hidden)\n",
    "    q_w_12_heads = q_w_12_heads.reshape(12, -1, 768)\n",
    "    k_w_12_heads = k_w_12_heads.reshape(12, -1, 768)\n",
    "    v_w_12_heads = v_w_12_heads.reshape(12, -1, 768)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    print(f\"The shape of query weight matrix after reshaping is {q_w_12_heads.shape}, key weight is {k_w_12_heads.shape}, and values weight is {v_w_12_heads.shape}\\n\")\n",
    "    # Shape here for each weight component is(d_head, d_hidden)\n",
    "    q_w_1_head = q_w_12_heads[0,:,:]\n",
    "    k_w_1_head = k_w_12_heads[0,:,:]\n",
    "    v_w_1_head = v_w_12_heads[0,:,:]\n",
    "\n",
    "    q_b_1_head = q_b_12_heads[0,:]\n",
    "    k_b_1_head = k_b_12_heads[0,:]\n",
    "    v_b_1_head = v_b_12_heads[0,:]\n",
    "\n",
    "    print(f\"The shape of query weight matrix after reshaping for one head is {q_w_1_head.shape}, key weight is {k_w_1_head .shape}, and values weight is {v_w_1_head .shape}\\n\")\n",
    "    print(f\"The shape of query bias matrix after reshaping for one head is {q_b_1_head.shape}, key bias is {k_b_1_head .shape}, and values bias is {v_b_1_head .shape}\\n\")\n",
    "\n",
    "\n",
    "    out_encoder_10 = attention_maps[0][0]\n",
    "    out_encoder_10 = out_encoder_10.unsqueeze(0)\n",
    "    # print(out_encoder_10.shape)\n",
    "\n",
    "\n",
    "    # place holder to get the attention weights from the heads to use it for later calculations\n",
    "    att_weights =[]\n",
    "    satt = []\n",
    "\n",
    "    # This loop is created to loop over the heads, in order to get all of the attention matrices (qk^{T}) per heads\n",
    "    for i in range(12):\n",
    "        q_w = q_w_12_heads[i,:,:]\n",
    "        k_w = k_w_12_heads[i,:,:]\n",
    "        v_w = v_w_12_heads[i,:,:]\n",
    "\n",
    "        q_b = q_b_12_heads[i,:]\n",
    "        k_b = k_b_12_heads[i,:]\n",
    "        v_b = v_b_12_heads[i,:]\n",
    "\n",
    "        \n",
    "\n",
    "        q = torch.matmul(out_encoder_10, q_w.T) \n",
    "        k = torch.matmul(out_encoder_10, k_w.T) \n",
    "        v = torch.matmul(out_encoder_10, v_w.T) \n",
    "\n",
    "        qk = torch.matmul(q, k.transpose(2, 1))/8\n",
    "        qk = torch.softmax(qk, dim=(2))\n",
    "        # print(qk.shape)\n",
    "        att_weights.append(qk)"
   ],
   "id": "b72e64555b3dfc45",
   "execution_count": 47,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T20:46:49.424821Z",
     "start_time": "2024-08-22T20:46:49.421742Z"
    }
   },
   "cell_type": "code",
   "source": "len(att_weights)",
   "id": "54dce2d0aa647380",
   "execution_count": 49,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T20:46:01.487839Z",
     "start_time": "2024-08-22T20:46:01.484786Z"
    }
   },
   "cell_type": "code",
   "source": "att_weights[0].shape",
   "id": "bfe1902803149af4",
   "execution_count": 48,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T20:46:58.487168Z",
     "start_time": "2024-08-22T20:46:58.485011Z"
    }
   },
   "cell_type": "code",
   "source": "attention_map = att_weights[11][0, 0]",
   "id": "b671b5127c1c16e2",
   "execution_count": 50,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T20:46:59.388611Z",
     "start_time": "2024-08-22T20:46:59.385955Z"
    }
   },
   "cell_type": "code",
   "source": "attention_map.shape",
   "id": "87afafdfac210363",
   "execution_count": 51,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T20:47:14.814630Z",
     "start_time": "2024-08-22T20:47:14.803388Z"
    }
   },
   "cell_type": "code",
   "source": "attention_map.reshape(24, 24)",
   "id": "d75dd06c66f14716",
   "execution_count": 53,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T20:47:22.737358Z",
     "start_time": "2024-08-22T20:47:22.734185Z"
    }
   },
   "cell_type": "code",
   "source": "np.sqrt(577)",
   "id": "705a5b3898f4f820",
   "execution_count": 54,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Choose a layer and a head to visualize\n",
    "layer_idx = 11  # Index of the layer (ViT-B/16 has 12 layers)\n",
    "head_idx = 0    # Index of the head (each layer has multiple heads, e.g., 12)\n",
    "\n",
    "# Get the attention map for the chosen layer and head\n",
    "attention_map = attention_maps[layer_idx][0, head_idx]  # [batch, head, tokens, tokens]\n",
    "\n",
    "# Visualize the attention map\n",
    "cls_attention = attention_map[:, 0]  # Attention to the [CLS] token\n",
    "cls_attention = cls_attention.reshape(14, 14)  # Reshape to 2D (for 14x14 patches)\n",
    "\n",
    "plt.imshow(cls_attention, cmap='viridis')\n",
    "plt.title(f'Layer {layer_idx + 1}, Head {head_idx + 1}')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ],
   "id": "9a7083d37be0f78c",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "(outputs > 0.5).sum()",
   "id": "38a8854da6440de6",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prediction = outputs[0].argmax(-1)\n",
    "print(f\"Predicted class: {vit_model.config.id2label[prediction.item()]} | {prediction.item()}\")"
   ],
   "id": "948b33f9cbef30df",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# get the attention from the last layer\n",
    "attentions = outputs.attentions[-1].squeeze(0)\n",
    "fig, ax = plt.subplots(4, 4, figsize=(15, 15))\n",
    "\n",
    "\n",
    "for i in range(12):\n",
    "    attention_map = attentions[i].detach().numpy()\n",
    "    \n",
    "    # iterate over the heads and obtain the CLS token\n",
    "    cls_attention_map = attention_map[0, 1:]\n",
    "    \n",
    "    # Now patches are 14 by 14 thus 196 needs to be reshaped first\n",
    "    cls_attention_map = cls_attention_map.reshape(14, 14)\n",
    "    \n",
    "    #Resize the heatmap to overlap over the image\n",
    "    cls_attention_map = cv2.resize(cls_attention_map, img.size)\n",
    "    \n",
    "    ax[i//3, 0].imshow(img)\n",
    "    ax[i//3, 0].axis('off')\n",
    "    ax[i//3, 0].set_title(f'Original Image of a: [{vit_model.config.id2label[prediction.item()]}]')\n",
    "    \n",
    "    # overlap attention map over image\n",
    "    ax[i//3, (i%3)+1].imshow(img)\n",
    "    ax[i//3, (i%3)+1].imshow(cls_attention_map, cmap='jet', alpha=0.4)\n",
    "    \n",
    "    ax[i//3, (i%3)+1].axis('off')\n",
    "    ax[i//3, (i%3)+1].set_title(f'Attention map - [Head: {i + 1}]')\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
